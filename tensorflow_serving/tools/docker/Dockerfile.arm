# Copyright 2018 Erik Maciejewski
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
ARG ARCH=arm64v8
ARG TFS_VERSION=latest
FROM tensorflow/serving:$TFS_VERSION-devel as devel
# install arm toolchain dependencies
COPY tensorflow_serving/tools/docker/install_arm_toolchain.sh /install_arm_toolchain.sh
RUN /install_arm_toolchain.sh
# overwrite existing sources
# adds config groupings with default options for arm32v7 and arm64v8
COPY tools/bazel.rc /tensorflow-serving/tools/bazel.rc
# uses bazelrc make variables to get arch specific flags
COPY third_party/libevent.BUILD /tensorflow-serving/third_party/libevent.BUILD
# implements bazel external source patching for aws client libs
COPY WORKSPACE /tensorflow-serving/WORKSPACE
COPY tensorflow_serving/repo.bzl /tensorflow-serving/tensorflow_serving/repo.bzl
COPY external/aws.BUILD.patch /tensorflow-serving/external/aws.BUILD.patch
# backport upstream patch for gzip implementation causing build failures for 32-bit
COPY tensorflow_serving/util/net_http/server/internal/evhttp_request.cc /tensorflow-serving/tensorflow_serving/util/net_http/server/internal/evhttp_request.cc
# add new sources
# adds bazel C++ toolchains for armhf and arm64
COPY tools/cpp /tensorflow-serving/tools/cpp

FROM devel as build
ARG ARCH
ARG BUILD_OPTS
RUN bazel build --verbose_failures --config=${ARCH} ${BUILD_OPTS} tensorflow_serving/model_servers:tensorflow_model_server

FROM ${ARCH}/debian:stretch-slim as tensorflow_model_server
# Install TF Serving binary
COPY --from=build /tensorflow-serving/bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server /usr/bin/tensorflow_model_server
# Expose ports
# gRPC
EXPOSE 8500
# REST
EXPOSE 8501
# Set where models should be stored in the container
ENV MODEL_BASE_PATH=/models
RUN mkdir -p ${MODEL_BASE_PATH}
# The only required piece is the model name in order to differentiate endpoints
ENV MODEL_NAME=model
ENTRYPOINT tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME}
